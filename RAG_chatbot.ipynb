{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f13b4b4511984ff6b293cec0f1623eb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94a9339448b0433eaac2fb0ed2a86e69",
            "placeholder": "​",
            "style": "IPY_MODEL_60be21f94bb84d0e8be8a0be72cfbcdc",
            "value": "<h2 style=\"color: #0065bd; font-family: Arial, sans-serif; margin-bottom: 20px;\">AI Research Assistant </h2>"
          }
        },
        "94a9339448b0433eaac2fb0ed2a86e69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60be21f94bb84d0e8be8a0be72cfbcdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4e3667302c5c483798e1d4c53391a421": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_a15b9791db704cf1a8feb463605d726c",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "\n",
                  "You (21:39): Please summarize Gnewuch et al. 2023\n",
                  "\n",
                  "Assistant (21:39): 1. **Context**: The study focuses on customer interactions with hybrid service agents, which combine human and AI elements in service delivery. It explores how these interactions affect customer satisfaction and service outcomes.\n",
                  "\n",
                  "2. **Research Question & Summary of Key Findings**: The research investigates how the presence of AI in service interactions influences customer perceptions and behaviors. Key findings suggest that customers exhibit varying levels of trust and satisfaction based on the hybrid nature of the service agent. Specifically, the study identifies that customers prefer human agents for complex inquiries but appreciate AI for efficiency in straightforward tasks.\n",
                  "\n",
                  "3. **Theme**: Human + AI Collaboration: The study emphasizes the roles of both human and AI agents in service interactions, highlighting how their collaboration can enhance customer experiences.\n",
                  "\n",
                  "4. **Method**: Empirical Study: The research employs empirical methods, including surveys and experiments, to gather data on customer interactions with hybrid service agents.\n",
                  "\n",
                  "5. **Contribution**: The study contributes to the understanding of hybrid service models by providing insights into customer preferences and the dynamics of human-AI collaboration in service contexts. It offers practical implications for businesses looking to optimize their service delivery through AI integration.\n",
                  "\n",
                  "6. **Future Directions & Limitations**: The research identifies gaps in understanding the long-term effects of hybrid service agents on customer loyalty and the potential for AI to handle more complex tasks. Future studies could explore these dimensions further and address the limitations of current AI capabilities in service settings.\n",
                  "\n",
                  "### References\n",
                  "- Gnewuch, U., et al. (2023). Customer Interactions with Hybrid Service Agents. Information Systems Research, Articles in Advance. DOI: 10.1287/mnsc.2023.4770.\n"
                ]
              }
            ]
          }
        },
        "a15b9791db704cf1a8feb463605d726c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": "1px solid #ddd",
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": "500px",
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": "auto",
            "padding": "20px",
            "right": null,
            "top": null,
            "visibility": null,
            "width": "800px"
          }
        },
        "a5e8c3c7ca194de1b41223582d9b5a40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_ccc259fa423c47cdab0d254c1dfbcfc6",
            "placeholder": "Type your message and press Enter...",
            "style": "IPY_MODEL_6084dfa71755439d96277e717bf87ca5",
            "value": ""
          }
        },
        "ccc259fa423c47cdab0d254c1dfbcfc6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": "1px solid #ddd",
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": "10px 0",
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": "10px",
            "right": null,
            "top": null,
            "visibility": null,
            "width": "800px"
          }
        },
        "6084dfa71755439d96277e717bf87ca5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"overflow: hidden;\">\n",
        "  <!-- Colab Button on the Left -->\n",
        "  <a href=\"https://colab.research.google.com/github/trashpanda-ai/TBD.ipynb\" target=\"_parent\">\n",
        "    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\" width=\"150\" style=\"float: left; margin-right: 15px;\" />\n",
        "  </a>\n",
        "  \n",
        "  <!-- Text on the Right -->\n",
        "  <div style=\"text-align: right;\">\n",
        "    <h3><span style=\"color:gray\"> GPT Tool for HAI Literature Review </span></h3>\n",
        "  </div>\n",
        "</div>\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "\n",
        "<h1><center>Retrieval Augmented Generation (RAG) Chatbot with OpenAI LLM and LangChain</center></h1>\n",
        "<h2><center> <span style=\"font-weight:normal\"><font color='#0065bd'> Take home assignment for an application </font>  </span></center></h2>\n",
        "\n",
        "\n",
        "<h3><center><font color='gray'>JONAS GOTTAL</font></center></h3>\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "C8epZ5--MbYP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPT-based Research Summarization and Literature Review Tool (RAG System)\n",
        "This solution leverages a Retrieval-Augmented Generation (RAG) framework, combining a collection of various research papers with an interactive GPT-powered chatbot to facilitate real-time querying and information retrieval. The system enables users to:\n",
        "\n",
        "- Summarize Research Articles: The tool stores and indexes the 32 reference papers, allowing the user to query specific aspects of each article. The chatbot can extract and summarize key details such as the research context, questions, findings, themes (like human vs. AI comparisons or human-AI collaboration), methods, contributions, and future directions, providing concise, on-demand summaries.\n",
        "- Generate Literature Reviews: The system allows users to input custom queries (e.g., \"I want all research on human vs. AI and empirical studies\"). Based on the query, the system retrieves the most relevant papers from the database, summarizes them, and synthesizes a literature review that discusses common themes, trends, and potential future research areas. The review is presented in coherent paragraphs, with citations formatted according to academic standards.\n",
        "\n",
        "By combining a rich database of research papers with an interactive chatbot interface, this RAG-powered tool offers an intuitive and dynamic way for users to engage with academic literature, ask detailed questions, and receive customized summaries and literature reviews in real-time."
      ],
      "metadata": {
        "id": "49HOXvzzPXEb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Detailed requirements\n",
        "#### Function 1: Guidance\n",
        "For each of the 32 reference papers (PDF), your GPT tool should extract the following information:\n",
        "1.    Context: Specify whether the study is focused on a specific industry, task or a broader, conceptual scope.\n",
        "2.    Research Question and Findings: Identify the main research question and summarise the key findings.\n",
        "3.    Theme of Research:\n",
        "  - Human vs. AI: Highlight any comparisons of comparative advantages between humans and AI, including condition-based results or scenarios where one outperforms the other.\n",
        "  - Human + AI Collaboration: Indicate the type of collaboration discussed, such as the roles of human and AI, the sequences of actions of human and AI taken in the process, and so on.\n",
        "4.    Method: Classify the study method as one of the following:\n",
        "  - Conceptual/Case Study\n",
        "  - Modeling: Either Stylized Modeling or Operations Research (OR) Model\n",
        "  - Empirical Study: Lab/Field Experiment or Secondary Data Analysis\n",
        "5.    Contribution: Identify the primary contribution of the study, categorizing it as theoretical, managerial, or methodological.\n",
        "6.    Future Potential and Limitations: Summarise what the study states about future research directions or the limitations of its findings.\n",
        "\n",
        "#### Function 2: Guidance\n",
        "If a user types \"I want all research to demonstrate 1) human VS AI research, and 2) empirical research,\" the system should:\n",
        "- Retrieve all relevant articles from the backend database that discuss human VS AI research and empirical research.\n",
        "- Generate summaries of these articles in the specified format.\n",
        "- Compile the summaries into coherent paragraphs that discuss how the research in these areas is connected, identifying common themes, trends, and potential future directions.\n",
        "- Provide a reference list for all retrieved articles, formatted according to standard academic citation styles.\n",
        "\n",
        "### Interpretation of requirements\n",
        "Since there is an interaction based on natural language queries, the implementation needs a variance of RAG. For this specific implementation, a LangChain database vector database for the RAG is used. The LLM is provided through OpenAIs API."
      ],
      "metadata": {
        "id": "DrrVM01PQknb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install all packages:"
      ],
      "metadata": {
        "id": "BMyoQ9-ATcHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive/')\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "!pip install langchain -q\n",
        "!pip install openai -q\n",
        "!pip install tiktoken -q\n",
        "!pip install faiss-gpu -q\n",
        "!pip install langchain_experimental -q\n",
        "!pip install unstructured -q\n",
        "!pip install \"unstructured[pdf]\"  -q\n"
      ],
      "metadata": {
        "id": "8rd4HJTqP8ug",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68cefacf-b7a6-43bc-ce5f-a5713f5f2e91"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.0/209.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.5/409.5 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m586.9/586.9 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.9/274.9 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.9/62.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.4/48.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.5/112.5 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.9/486.9 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Upload your literature folder as zip file:"
      ],
      "metadata": {
        "id": "s0ShxNjqCpJD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get folder of PDFs\n",
        "# unfortunately too large for github and google drive...\n",
        "!wget https://syncandshare.lrz.de/dl/fiRNoSMHyQvo58L8YKuK3b/literature.zip\n",
        "!wget https://raw.githubusercontent.com/trashpanda-ai/RAG-chatbot/refs/heads/main/__init__.py\n",
        "!wget https://raw.githubusercontent.com/trashpanda-ai/RAG-chatbot/refs/heads/main/rag.py\n",
        "!unzip literature.zip"
      ],
      "metadata": {
        "id": "DgAxP9ya-ezz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a9d9870-1e87-44ea-9393-a5fe5964c695"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-29 21:12:21--  https://raw.githubusercontent.com/trashpanda-ai/RAG-chatbot/refs/heads/main/__init__.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 0 [text/plain]\n",
            "Saving to: ‘__init__.py’\n",
            "\n",
            "\r__init__.py             [<=>                 ]       0  --.-KB/s               \r__init__.py             [ <=>                ]       0  --.-KB/s    in 0s      \n",
            "\n",
            "2024-11-29 21:12:21 (0.00 B/s) - ‘__init__.py’ saved [0/0]\n",
            "\n",
            "--2024-11-29 21:12:21--  https://raw.githubusercontent.com/trashpanda-ai/RAG-chatbot/refs/heads/main/rag.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6483 (6.3K) [text/plain]\n",
            "Saving to: ‘rag.py’\n",
            "\n",
            "rag.py              100%[===================>]   6.33K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-11-29 21:12:21 (64.5 MB/s) - ‘rag.py’ saved [6483/6483]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# if the exact same data is used, you can \"recycle\" the vector database which speeds up the start time\n",
        "#!wget https://syncandshare.lrz.de/dl/fi9E4ZMTV2QEaNADSkazuq/vectorstore.zip\n",
        "#!unzip vectorstore.zip"
      ],
      "metadata": {
        "id": "GQjmKBLX4JSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import the chatbot from ```rag.py```:"
      ],
      "metadata": {
        "id": "ncaGstvOCxkP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from rag import SimpleRAGChat\n",
        "import os"
      ],
      "metadata": {
        "id": "iBiPlwnsP85I"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Enter your API Key:\n",
        "(if you dont have one, reach out to me and I'll provide one for testing purpose)"
      ],
      "metadata": {
        "id": "1HdQeRpXC7OQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt the user for their OpenAI API key\n",
        "api_key = input(\"Please enter your OpenAI API key: \")\n",
        "\n",
        "# Set the API key as an environment variable\n",
        "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "print(\"OPENAI_API_KEY has been set!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rf8LMffhXC-m",
        "outputId": "fef3d4a9-c5ff-4be1-f217-a52ef7949da8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter your OpenAI API key: NotYourKey\n",
            "OPENAI_API_KEY has been set!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's go!"
      ],
      "metadata": {
        "id": "AFHjlvA7DH8H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat = SimpleRAGChat()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697,
          "referenced_widgets": [
            "f13b4b4511984ff6b293cec0f1623eb6",
            "94a9339448b0433eaac2fb0ed2a86e69",
            "60be21f94bb84d0e8be8a0be72cfbcdc",
            "4e3667302c5c483798e1d4c53391a421",
            "a15b9791db704cf1a8feb463605d726c",
            "a5e8c3c7ca194de1b41223582d9b5a40",
            "ccc259fa423c47cdab0d254c1dfbcfc6",
            "6084dfa71755439d96277e717bf87ca5"
          ]
        },
        "id": "2zXMT4rx4woo",
        "outputId": "d22237fa-f928-41c8-c182-1ec0ebef4025"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTML(value='<h2 style=\"color: #0065bd; font-family: Arial, sans-serif; margin-bottom: 20px;\">AI Research Assis…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f13b4b4511984ff6b293cec0f1623eb6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output(layout=Layout(border='1px solid #ddd', height='500px', overflow_y='auto', padding='20px', width='800px'…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4e3667302c5c483798e1d4c53391a421"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Text(value='', layout=Layout(border='1px solid #ddd', margin='10px 0', padding='10px', width='800px'), placeho…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a5e8c3c7ca194de1b41223582d9b5a40"
            }
          },
          "metadata": {}
        }
      ]
    }
  ]
}